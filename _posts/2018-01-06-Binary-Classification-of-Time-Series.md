
# Binary Classification of Time Series Data in Python

## Motivation
You probably know this situation: You open a website and the server is down. Is there a way to prevent this from happening? What if we could detect when a server is overloaded and then shift some work to another server? System monitoring allows us to measure important variables as system CPU load, memory usage or garbage collection. This data could then be used to predict whether a server will shutdown in the next hour or not.  
The problem posed can be reduced to binary classification (shutdown or not) of time series (system monitoring data). This tutorial will show you how to perform binary classification of time series on a toy problem and outline how to apply it to predict shutdowns. 

## Table of Contents
1. General Setup
1. Create Train and Test Data
1. Feature Extraction
1. Classification
1. Application: How to use System Monitoring Data to Predict Server Shutdowns

## General setup
There are several ways to use time series for binary classification. Here we use feature extraction to capture important characteristics of our time series in one dimensional statistics (as the max, variance, mean slope etc.) and use the derived feature vector to learn our classification model. 
The package TSFRESH [1] can be used to automatically extract hundreds of features of a time series and to statistically test which of those features are relevant to distinguish between classes. This makes it highly useful for feature extraction. The classification will be implemented with the machine learning library of Apache Spark [2]. 

For the consecutive sections we use the following initialization of python.


```python
import seaborn
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tsfresh import *
import findspark
findspark.init()
from pyspark.mllib.tree import DecisionTree, DecisionTreeModel
from pyspark.mllib.regression import LabeledPoint
from pyspark import SparkContext
sc = SparkContext("local", "App")
np.random.seed(1)
```

![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/general_setup.png)
**Figure 1:** General Setup: Extracting statistical features from time series data for classification.

## Create Train and Test Data
To use time series for feature extraction with TSFRESH we need to bring the data and class information into the following format: 

| index | id | timeseries type 1 | timeseries type 2 |
|-------|:--:|:-----------------:|:-----------------:|
| 0     |  1 |        0.10       |         12        |
| 1     |  1 |        0.24       |         3         |
| 2     |  1 |        0.31       |         2         |
| 3     |  2 |        0.29       |         6         |
| 4     |  2 |        0.32       |         5         |
| 5     |  2 |        0.33       |         5         |
**Table 1: ** Format of the time series data (pandas DataFrame).

| id | class |
|----|:-----:|
| 1  |   0   |
| 2  |   1   |
**Table 2: ** Format of id to class mapping (pandas Series).

For each class of our toy data a different mean slope is chosen. Several time series are computed per class: Starting at point 0, the slope at each time step is generated by sampling from a normal distribution around the given mean.  The  standard deviation is the same for both classes---increasing it, simply makes the classification more difficult.


```python
def create_data(n_timeseries, timeseries_len, slope_mean_1, slope_mean_0, slope_std):
    data_0, labels_0 = create_timeseries(n_timeseries, timeseries_len, slope_mean_0, slope_std, range(n_timeseries), 0)
    data_1, labels_1 = create_timeseries(n_timeseries, timeseries_len, slope_mean_1, slope_std,
                                         range(n_timeseries, 2*n_timeseries), 1)

    data = pd.concat([data_0, data_1], axis=0, ignore_index=True)
    labels = pd.concat([labels_0, labels_1], axis=0)
    return data, labels


def create_timeseries(n_timeseries, timeseries_len, slope_mean, slope_std, ids_per_timeseries, label):
    ids = list()
    timeseries = list()
    labels = dict()
    for i in range(n_timeseries):
        timeseries_tmp = np.cumsum(np.random.normal(slope_mean, slope_std, timeseries_len))
        timeseries.extend(timeseries_tmp)
        ids_tmp = [ids_per_timeseries[i]] * timeseries_len
        ids.extend(ids_tmp)
        labels[ids_per_timeseries[i]] = label
    data = pd.DataFrame({'id': ids, 'timeseries': timeseries})
    labels = pd.Series(labels)
    return data, labels


def plot_data(data, title):
    plt.figure()
    for i in range(n_timeseries):
        plt.plot(range(timeseries_len),
                data['timeseries'].iloc[i * timeseries_len:(i + 1) * timeseries_len].values, 'b',
                 label="Class 0" if i == 0 else "")
    for i in range(n_timeseries, 2 * n_timeseries - 1):
        plt.plot(range(timeseries_len),
                data['timeseries'].iloc[i * timeseries_len:(i + 1) * timeseries_len].values, 'r',
                 label="Class 1" if i == n_timeseries else "")
    plt.legend(loc='upper left', fontsize=16)
    plt.xlabel('Time', fontsize=16)
    plt.ylabel('Time series', fontsize=16)
    plt.title(title, fontsize=16)
    plt.show()
```

Now we can create some train and test data and see what they look like.


```python
n_timeseries = 20
timeseries_len = 40
slope_mean_1 = 1
slope_mean_0 = 0
slope_std = 3

train_data, train_labels = create_data(n_timeseries, timeseries_len, slope_mean_1, slope_mean_0, slope_std)
test_data, test_labels = create_data(n_timeseries, timeseries_len, slope_mean_1, slope_mean_0, slope_std)

plot_data(train_data, 'Train data')
plot_data(test_data, 'Test data')
```

![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/train_data.png)
![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/test_data.png)

By eye it is quite difficult to classify all time series into the two classes.

## Feature Extraction
As we already brought our data into the right format, we can directly call *extract_relevant_features* from the package TSFRESH with our train data and labels. The function *extract_relevant_features* will compute all features implemented in TSFRESH from each time series in the data and subsequently test each feature for significance (see [1] for details). It returns a DataFrame with all relevant features in the columns and the values of each sample in the rows.


```python
train_features = extract_relevant_features(train_data, train_labels, column_id='id')
```

```python
train_features.head()
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>variable</th>
      <th>timeseries__fft_coefficient__coeff_13__attr_"real"</th>
      <th>timeseries__linear_trend__attr_"slope"</th>
      <th>timeseries__time_reversal_asymmetry_statistic__lag_3</th>
      <th>timeseries__agg_linear_trend__f_agg_"min"__chunk_len_5__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_5__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_10__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"max"__chunk_len_10__attr_"slope"</th>
      <th>timeseries__time_reversal_asymmetry_statistic__lag_2</th>
      <th>timeseries__fft_coefficient__coeff_12__attr_"real"</th>
      <th>timeseries__agg_linear_trend__f_agg_"max"__chunk_len_5__attr_"slope"</th>
      <th>...</th>
      <th>timeseries__change_quantiles__f_agg_"mean"__isabs_False__qh_0.8__ql_0.4</th>
      <th>timeseries__number_peaks__n_1</th>
      <th>timeseries__change_quantiles__f_agg_"var"__isabs_True__qh_0.8__ql_0.4</th>
      <th>timeseries__energy_ratio_by_chunks__num_segments_10__segment_focus_3</th>
      <th>timeseries__fft_coefficient__coeff_20__attr_"angle"</th>
      <th>timeseries__fft_coefficient__coeff_4__attr_"angle"</th>
      <th>timeseries__agg_autocorrelation__f_agg_"median"</th>
      <th>timeseries__fft_aggregated__aggtype_"centroid"</th>
      <th>timeseries__index_mass_quantile__q_0.1</th>
      <th>timeseries__autocorrelation__lag_3</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6.128558</td>
      <td>-0.375272</td>
      <td>-403.359040</td>
      <td>-1.977944</td>
      <td>-1.891858</td>
      <td>-3.661655</td>
      <td>-3.945440</td>
      <td>-254.409320</td>
      <td>8.927234</td>
      <td>-1.787924</td>
      <td>...</td>
      <td>-0.244533</td>
      <td>8.0</td>
      <td>1.044554</td>
      <td>0.048859</td>
      <td>0.0</td>
      <td>-53.808295</td>
      <td>-0.079544</td>
      <td>4.573621</td>
      <td>0.275</td>
      <td>0.543872</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.937265</td>
      <td>0.742989</td>
      <td>957.545471</td>
      <td>3.789694</td>
      <td>3.678779</td>
      <td>7.354160</td>
      <td>6.959161</td>
      <td>579.607045</td>
      <td>-5.472537</td>
      <td>3.772612</td>
      <td>...</td>
      <td>1.045119</td>
      <td>7.0</td>
      <td>1.179200</td>
      <td>0.003550</td>
      <td>180.0</td>
      <td>69.768402</td>
      <td>-0.619002</td>
      <td>3.476632</td>
      <td>0.325</td>
      <td>0.780687</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-9.495710</td>
      <td>0.061965</td>
      <td>-6.099019</td>
      <td>0.120589</td>
      <td>0.305859</td>
      <td>0.758906</td>
      <td>1.066851</td>
      <td>-3.014239</td>
      <td>-5.162243</td>
      <td>0.320524</td>
      <td>...</td>
      <td>0.186393</td>
      <td>10.0</td>
      <td>1.155615</td>
      <td>0.128016</td>
      <td>180.0</td>
      <td>-95.361774</td>
      <td>-0.153800</td>
      <td>3.813255</td>
      <td>0.275</td>
      <td>0.491764</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.765687</td>
      <td>0.441409</td>
      <td>844.506585</td>
      <td>2.151954</td>
      <td>2.189373</td>
      <td>4.195104</td>
      <td>3.628707</td>
      <td>486.193891</td>
      <td>8.221721</td>
      <td>1.997615</td>
      <td>...</td>
      <td>1.180730</td>
      <td>9.0</td>
      <td>0.837562</td>
      <td>0.028324</td>
      <td>180.0</td>
      <td>124.455257</td>
      <td>-0.261624</td>
      <td>3.444401</td>
      <td>0.225</td>
      <td>0.612517</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-14.642629</td>
      <td>0.924698</td>
      <td>3154.565522</td>
      <td>4.400194</td>
      <td>4.572208</td>
      <td>9.151209</td>
      <td>8.754106</td>
      <td>2164.534492</td>
      <td>-18.849873</td>
      <td>4.737662</td>
      <td>...</td>
      <td>0.849795</td>
      <td>10.0</td>
      <td>1.148852</td>
      <td>0.027355</td>
      <td>0.0</td>
      <td>117.367913</td>
      <td>-0.195863</td>
      <td>4.007377</td>
      <td>0.375</td>
      <td>0.764757</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 167 columns</p>
</div>



In order to compute the same features on the test set, we use the generic *extract\_features* method of TSFRESH and specify the parameter *kind\_to\_fc_parameters*.


```python
features = train_features.columns.values
feature_settings = feature_extraction.settings.from_columns(features)
test_features = extract_features(test_data, column_id='id', kind_to_fc_parameters=feature_settings)
test_features = test_features[train_features.columns.values]  # setting kind_to_fc_parameters did not work completely e.g. 'timeseries__friedrich_coefficients__m_3__r_30__coeff_1' is computed for more coeff than in train set
```


```python
test_features.head()
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>variable</th>
      <th>timeseries__fft_coefficient__coeff_13__attr_"real"</th>
      <th>timeseries__linear_trend__attr_"slope"</th>
      <th>timeseries__time_reversal_asymmetry_statistic__lag_3</th>
      <th>timeseries__agg_linear_trend__f_agg_"min"__chunk_len_5__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_5__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_10__attr_"slope"</th>
      <th>timeseries__agg_linear_trend__f_agg_"max"__chunk_len_10__attr_"slope"</th>
      <th>timeseries__time_reversal_asymmetry_statistic__lag_2</th>
      <th>timeseries__fft_coefficient__coeff_12__attr_"real"</th>
      <th>timeseries__agg_linear_trend__f_agg_"max"__chunk_len_5__attr_"slope"</th>
      <th>...</th>
      <th>timeseries__change_quantiles__f_agg_"mean"__isabs_False__qh_0.8__ql_0.4</th>
      <th>timeseries__number_peaks__n_1</th>
      <th>timeseries__change_quantiles__f_agg_"var"__isabs_True__qh_0.8__ql_0.4</th>
      <th>timeseries__energy_ratio_by_chunks__num_segments_10__segment_focus_3</th>
      <th>timeseries__fft_coefficient__coeff_20__attr_"angle"</th>
      <th>timeseries__fft_coefficient__coeff_4__attr_"angle"</th>
      <th>timeseries__agg_autocorrelation__f_agg_"median"</th>
      <th>timeseries__fft_aggregated__aggtype_"centroid"</th>
      <th>timeseries__index_mass_quantile__q_0.1</th>
      <th>timeseries__autocorrelation__lag_3</th>
    </tr>
    <tr>
      <th>id</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.374666</td>
      <td>-0.928612</td>
      <td>-1192.552896</td>
      <td>-4.573194</td>
      <td>-4.660014</td>
      <td>-9.498714</td>
      <td>-9.712750</td>
      <td>-825.240960</td>
      <td>12.000443</td>
      <td>-4.656872</td>
      <td>...</td>
      <td>-1.894590</td>
      <td>9.0</td>
      <td>4.356062</td>
      <td>0.012249</td>
      <td>0.0</td>
      <td>-79.456853</td>
      <td>-0.691135</td>
      <td>3.152025</td>
      <td>0.425</td>
      <td>0.871547</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.692521</td>
      <td>0.200101</td>
      <td>54.728957</td>
      <td>0.515773</td>
      <td>0.969620</td>
      <td>1.775886</td>
      <td>3.006548</td>
      <td>78.964897</td>
      <td>-0.017534</td>
      <td>1.170861</td>
      <td>...</td>
      <td>-0.599897</td>
      <td>11.0</td>
      <td>1.489682</td>
      <td>0.153287</td>
      <td>180.0</td>
      <td>63.951231</td>
      <td>0.059318</td>
      <td>6.561858</td>
      <td>0.200</td>
      <td>0.417464</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.519578</td>
      <td>-0.089679</td>
      <td>-18.126060</td>
      <td>-0.608508</td>
      <td>-0.452849</td>
      <td>-0.645900</td>
      <td>0.272146</td>
      <td>-20.839825</td>
      <td>6.197587</td>
      <td>-0.747013</td>
      <td>...</td>
      <td>-0.112424</td>
      <td>13.0</td>
      <td>1.111040</td>
      <td>0.259362</td>
      <td>0.0</td>
      <td>-76.424627</td>
      <td>-0.049868</td>
      <td>6.385188</td>
      <td>0.175</td>
      <td>0.224463</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-7.240918</td>
      <td>0.340649</td>
      <td>86.150039</td>
      <td>1.849879</td>
      <td>1.701541</td>
      <td>3.562722</td>
      <td>4.131888</td>
      <td>41.292260</td>
      <td>-17.984787</td>
      <td>1.700265</td>
      <td>...</td>
      <td>-0.682571</td>
      <td>11.0</td>
      <td>2.246103</td>
      <td>0.186884</td>
      <td>180.0</td>
      <td>-6.989059</td>
      <td>-0.221639</td>
      <td>3.453323</td>
      <td>0.300</td>
      <td>0.611155</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-20.386826</td>
      <td>0.785392</td>
      <td>2739.471322</td>
      <td>3.203955</td>
      <td>3.935282</td>
      <td>7.790772</td>
      <td>11.600346</td>
      <td>2737.523267</td>
      <td>-11.664610</td>
      <td>4.401523</td>
      <td>...</td>
      <td>0.837491</td>
      <td>7.0</td>
      <td>1.274570</td>
      <td>0.047821</td>
      <td>180.0</td>
      <td>106.918193</td>
      <td>-0.278995</td>
      <td>5.192927</td>
      <td>0.275</td>
      <td>0.695884</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 167 columns</p>
</div>



Now we can plot the distribution of class 0 and 1 for some feature and see how it separates them. Indeed, there are features like the following which for the most part distinguish between both classes.


```python
def plot_feature(features, labels, i_column, title):
    label_dict = labels.to_dict()
    labels_arr = np.array([label_dict[id] for id in train_features.index])
    
    width = 0.5
    fig, ax = plt.subplots()
    ax.boxplot(features.iloc[labels_arr == 0, i_column].values, positions=[0])
    ax.boxplot(features.iloc[labels_arr == 1, i_column].values, positions=[1])
    ax.set_xlim(0 - width, 1 + 2 * width)
    ax.set_xticks([0, 1])
    ax.set_xticklabels(['Class 0', 'Class 1'], fontsize=16)
    ax.set_ylabel(features.columns[i_column].replace('__', '\n'), fontsize=16)
    ax.set_title(title, fontsize=16)
    plt.show()

i_column = np.where('timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_5__attr_"slope"'==train_features.columns.values)[0][0]
plot_feature(train_features, train_labels, i_column, 'Train Data')
plot_feature(test_features, test_labels, i_column, 'Test Data')
```

![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/feature_train.png)
![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/feature_test.png)

For the classification we need to add the class in front of each sample. Furthermore we should make sure that the columns of the train and test set are in the same order. In the end we save both as csv file.


```python
def prepare_for_classification(features, labels):
    labels_dict = labels.to_dict()
    features.index = [labels_dict[id] for id in features.index.values]  # put class into the index
    features.columns = [str(c) for c in
                        features.columns.values]  # ensure all column names are str (otherwise sort_index will break)
    features.sort_index(axis=1, inplace=True)    
    return features
    
train_features = prepare_for_classification(train_features, train_labels)
test_features = prepare_for_classification(test_features, test_labels)
train_features.to_csv('./train_set.csv')
test_features.to_csv('./test_set.csv')
```

## Classification
We will do the classification with pyspark. For this reason we read the csv file with *SparkContext.textFile* which returns a Resilient Distributed Dataset (Sparks abstraction of a dataset).
After we removed the header, each sample is transformed to a *LabeledPoint*, an object containing the class and feature vector. 


```python
def to_labeledpoint(row):
    values = [float(x) for x in row.split(',')]
    return LabeledPoint(values[0], values[1:])

def read_data(save_dir):
    data = sc.textFile(save_dir)
    header = data.first()
    data = data.filter(lambda line: line != header)
    data_transformed = data.map(to_labeledpoint)
    return data_transformed
    
train_set = read_data('./train_set.csv')
test_set = read_data('./test_set.csv')
```

Different procedures can be used for the classification as Support Vector Machines (SVM), Logistic Regression or Decision Trees. As an example we apply Decision Trees here. 


```python
model = DecisionTree.trainClassifier(train_set, numClasses=2, categoricalFeaturesInfo={},
                                     impurity='gini', maxDepth=1, maxBins=100)
```

For the evaluation we will use the following confusion matrix: 


|                       |  Class 1 True  |        Class 0 True       |             |
|:---------------------:|:--------------:|:-------------------------:|-------------|
| **Class 1 Predicted** |  True Positive |       False Negative      | Recall      |
| **Class 0 Predicted** | False Positive |       True Negative       | Specificity |
|                       |    Precision   | Negative Predictive Value | Accuracy    |
                  

```python
def compute_confusion_matrix(label_and_prediction):
    true_positive = label_and_prediction.map(lambda lp: lp[0] == 1 and lp[1] == 1).sum()
    false_positive = label_and_prediction.map(lambda lp: lp[0] == 1 and lp[1] == 0).sum()
    true_negative = label_and_prediction.map(lambda lp: lp[0] == 0 and lp[1] == 0).sum()
    false_negative = label_and_prediction.map(lambda lp: lp[0] == 0 and lp[1] == 1).sum()
    recall = divide0(true_positive, true_positive + false_negative)
    precision = divide0(true_positive, true_positive + false_positive)
    specificity = divide0(true_negative, true_negative + false_positive)
    negative_predictive_value = divide0(true_negative, true_negative + false_negative)
    accuracy = divide0(true_positive + true_negative, true_positive + true_negative + false_positive + false_negative)

    confusion_matrix = [[true_positive, false_negative, recall],
                        [false_positive, true_negative, specificity],
                        [precision, negative_predictive_value, accuracy]]
    return confusion_matrix

def divide0(x, y):
    if y != 0:
        return x / y
    else:
        return 0
```

So lets look at the prediction of the learned model.


```python
predictions = model.predict(train_set.map(lambda x: x.features))
label_and_prediction = train_set.map(lambda lp: lp.label).zip(predictions)
print('Confusion Matrix Training: \n' + str(np.matrix((compute_confusion_matrix(label_and_prediction)))))

predictions = model.predict(test_set.map(lambda x: x.features))
label_and_prediction = test_set.map(lambda lp: lp.label).zip(predictions)
print('Confusion Matrix Testing: \n' + str(np.matrix((compute_confusion_matrix(label_and_prediction)))))
```

Training | 1 True | 0 True | 
:------: | :----: | :----: |
**1 Predicted** | 20 | 4 | 0.83  
**0 Predicted** | 1 | 16 | 1.00  
 | 1.00 | 0.80 | 0.90
 
Testing | 1 True | 0 True |
:-----: | :----: | :----: |
**1 Predicted** | 19 | 1 | 0.95
**0 Predicted** | 1 | 19 | 0.95  
 | 0.95 | 0.95 | 0.95
 
We see that the performance of the model is quite good on the train and on the test set with an accuracy of 0.9 and 0.95, respectively. For an even distribution of classes in a data set, the accuracy is the decisive factor. For skewed data sets as in the following shutdown prediction example precision and recall are very important as well.

We can also print the Decision tree, showing which features were used for the classification and at which values the splits were made. 


```python
print(model.toDebugString())
train_features.columns[11]
```

    DecisionTreeModel classifier of depth 1 with 3 nodes
      If (feature 11 <= 1.7035787613089959)
       Predict: 0.0
      Else (feature 11 > 1.7035787613089959)
       Predict: 1.0


    'timeseries__agg_linear_trend__f_agg_"mean"__chunk_len_5__attr_"slope"'



If we look up the feature in the columns at index 11, we find *timeseries\__agg_linear_trend\__f_agg_\_"mean"\__chunk_len_5\__attr_\_"slope"* (the feature we were plotting above). This features is computed in the following way: 
i) divide the time series in chunks of length 5
ii) compute the mean of each chunk
iii) do a linear regression on the chunk means
iv) take the slope of the linear regression  
So the decision rule makes sense because the feature is essentially the slope of the "downsampled" time series which was the attribute that we used to distinguish between the two classes.

## Application: How to use system monitoring data to predict server shutdowns
 System monitoring data as CPU load, memory usage or garbage collection etc. measured over time are essentially time series. So we can use them for classifying whether a shutdown will happen in the next hour or not. 
 It could for instance be conceivable that the shutdown was preceded by an increase of CPU load or that a maximum value of memory usage was exceeded. 

To apply the previous methods to this problem we only need to bring the data into the right format for the feature extraction and then do the same as above. 
Since we want to check for shutdowns continuously, we can use a sliding window approach. 

![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/time_series.png)

We stop for instance every half an hour (*= &Delta; window*) to make a prediction using the data from some hours ago up to this time point (*= window size*). The sample is assigned to class 1 (shutdown will happen) if a shutdown occurs in the next hour (*= &Delta; prediction*) and to class 0 (no shutdown will happen) otherwise.
 Having this division we can start the feature extraction and classification.

![png]({{ site.url }}{{ site.baseurl }}/images/Binary-Classification-of-Time-Series/sliding_window.png)

## Summary
In this post you learned how to do binary classification of time series in Python using the package TSFRESH and pyspark. It involved basically three steps:
* Data transformation
* Feature extraction
* Classification   

This general procedure can be applied to a lot of problems, one being the prediction of shutdowns using system monitoring data. So now you can measure your CPU and memory usage and start predicting.

## References
[1] Christ, M., Kempa-Liehr, A.W. and Feindt, M. (2016).
 Distributed and parallel time series feature extraction for industrial big data applications.
 *ArXiv e-print 1610.07717*, https://arxiv.org/abs/1610.07717.   
[2] Meng, X. et al. (2016). 
 Mllib: Machine learning in apache spark. 
 Journal of Machine Learning Research, 17(34), 1-7.
